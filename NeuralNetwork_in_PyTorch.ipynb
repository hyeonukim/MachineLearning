{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jWB8moSjywe"
      },
      "source": [
        "# **CMPT 726/419 A3 Q4: Neural Networks in PyTorch**\n",
        "\n",
        "Do not edit any cells until told to do soâ€”the ones directly below should not be changed so you can access the required data and model for this problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOxA_2m94JJy"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import csv\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvdWYh6fiAFg"
      },
      "outputs": [],
      "source": [
        "class XYDataset(Dataset):\n",
        "    \"\"\"A basic dataset where the underlying data is a list of (x,y) tuples. Data\n",
        "    returned from the dataset should be a (transform(x), y) tuple.\n",
        "    Args:\n",
        "    source      -- a list of (x,y) data samples\n",
        "    transform   -- a torchvision.transforms transform\n",
        "    \"\"\"\n",
        "    def __init__(self, source, transform=transforms.ToTensor()):\n",
        "        super(XYDataset, self).__init__()\n",
        "        self.source = source\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): return len(self.source)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        x,y = self.source[idx]\n",
        "        return self.transform(x), y\n",
        "\n",
        "def build_dataset():\n",
        "    \"\"\"Returns the subset of the CIFAR-10 dataset containing only horses and\n",
        "    deer, with the labels for each class modified to zero and one respectively.\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomResizedCrop(32, scale=(0.75, 1.0)),\n",
        "        transforms.ToTensor()])\n",
        "\n",
        "    data = CIFAR10(root=\".\", train=True, download=True)\n",
        "    data = [(x, (0 if y == 2 else 1)) for x,y in data if y in {2, 5}]\n",
        "    return XYDataset(data, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-RsUADD1PBB"
      },
      "outputs": [],
      "source": [
        "def generate_test_predictions(model):\n",
        "    \"\"\"Generates test predictions using [model].\"\"\"\n",
        "    data_te = torch.load(\"cifar2_te.pt\")\n",
        "    loader = DataLoader(data_te, batch_size=128, num_workers=6, shuffle=False)\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for x,_ in loader:\n",
        "            fx = model(x.to(device))\n",
        "            preds += (fx > .5).float().view(-1).cpu().tolist()\n",
        "\n",
        "    with open(\"test_predictions.csv\", \"w+\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Id\", \"Category\"])\n",
        "        for idx,p in enumerate(preds):\n",
        "            writer.writerow([str(idx), str(int(p))])\n",
        "    tqdm.write(\"Wrote model predictions to test_predictions.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwNWWjBHeRYu"
      },
      "outputs": [],
      "source": [
        "class NN(nn.Module):\n",
        "    \"\"\"A simple ConvNet for binary classification.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(NN, self).__init__()\n",
        "        self.c1 = nn.Conv2d(3, 32, kernel_size=3)\n",
        "        self.c2 = nn.Conv2d(32, 32, kernel_size=3)\n",
        "        self.fc = nn.Linear(25088, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs = len(x)\n",
        "        fx = self.c1(x)\n",
        "        fx = self.relu(fx)\n",
        "        fx = self.c2(fx)\n",
        "        fx = self.relu(fx)\n",
        "        fx = fx.view(bs, -1)\n",
        "        fx = self.fc(fx)\n",
        "        fx = self.sigmoid(fx)\n",
        "        return fx.view(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvCUQ8AJkZf1"
      },
      "source": [
        "\n",
        "### ----- EDIT NO CODE ABOVE THIS CELL -----\n",
        "### ----- EDIT CODE BENEATH THIS CELL -----\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtA0_XyQyBfW"
      },
      "outputs": [],
      "source": [
        "def validate(model, loader):\n",
        "    \"\"\"Returns a (acc_val, loss_val) tuple, where [acc_val] and [loss_val] are\n",
        "    respectively the validation accuracy and loss of [model] on the data in\n",
        "    [loader].\n",
        "\n",
        "    Args:\n",
        "    model   -- a model, already moved onto the GPU\n",
        "    loader  -- a DataLoader over validation data\n",
        "    \"\"\"\n",
        "    acc_val, loss_val = 0, 0\n",
        "    loss_fn = nn.BCELoss(reduction=\"mean\")\n",
        "    ##### YOUR CODE STARTS HERE (redefine [acc_val] and [loss_val] somewhere) ##\n",
        "\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for data in loader:\n",
        "        inputs, labels = data\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels.float())\n",
        "        loss_val += loss.item()\n",
        "        predicted = (outputs > .5).float()\n",
        "        total+= labels.size(0)\n",
        "        acc_val += (predicted == labels).sum()\n",
        "\n",
        "    acc_val = float(acc_val)/float(total) * 100\n",
        "\n",
        "    ##### YOUR CODE ENDS HERE   ################################################\n",
        "    return acc_val, loss_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6iPZe_ZlCCa"
      },
      "outputs": [],
      "source": [
        "def one_epoch(model, optimizer, loader):\n",
        "    \"\"\"Returns a (model, optimizer, avg_loss) tuple after training [model] on\n",
        "    the data in [loader] for one epoch.\n",
        "\n",
        "    Args:\n",
        "    model       -- the neural network to train, already moved onto the GPU\n",
        "    optimizer   -- the optimizer used to train [model]\n",
        "    loader      -- a DataLoader with data to train [model] on\n",
        "\n",
        "    Returns\n",
        "    model       -- [model] after training for one epoch\n",
        "    optimizer   -- the optimizer used to train [model]\n",
        "    avg_loss    -- the average loss of [model] on each batch of [loader]\n",
        "    \"\"\"\n",
        "    avg_loss = 0\n",
        "    loss_fn = nn.BCELoss(reduction=\"mean\")\n",
        "    ##### YOUR CODE STARTS HERE (redefine [avg_loss] somewhere) ################\n",
        "\n",
        "    for i, data in enumerate(loader, 0):\n",
        "      inputs, labels = data\n",
        "      outputs = model(inputs)\n",
        "      optimizer.zero_grad()\n",
        "      loss = loss_fn(outputs, labels.float())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      avg_loss += loss.item()\n",
        "    \n",
        "    avg_loss = avg_loss/len(loader)\n",
        "\n",
        "    ##### YOUR CODE ENDS HERE   ################################################\n",
        "    return model, optimizer, avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1soS9WvG0LG"
      },
      "source": [
        "Next, write a function to take a set of hyperparameters and return a `(model, acc_val)` tuple with the model having been trained with the hyperparameters and `acc_val` being the validation accuracy of the model after training.\n",
        "\n",
        "Note the `**kwargs` in the function definition. This means that the function can take\n",
        "any number of keyword arguments as input, and they will be accessible inside the function despite not being specified in the function definition, and they will be accessible within dictionary named `kwargs` inside the function.\n",
        "\n",
        "_By passing in arguments this way, you can define whatever hyperparameters you want to use and pass them in!_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5jkETP4EdDH"
      },
      "outputs": [],
      "source": [
        "def train_and_validate(data_tr, data_val, **hyperparameters):\n",
        "    \"\"\"Returns a (model, acc_val) tuple where [model] is a neural\n",
        "    network of the NN class trained with [hyperparameters] on [data_tr] and \n",
        "    validated on [data_val], and [acc_val] is the validation accuracy of the\n",
        "    model after training.\n",
        "\n",
        "    Args:\n",
        "    data_tr         -- Dataset of training data\n",
        "    data_val        -- Dataset of validation data\n",
        "    hyperparameters -- kwarg dictionary of hyperparameters\n",
        "    \"\"\"\n",
        "    model, acc_val = NN(), 0\n",
        "    ##### YOUR CODE STARTS HERE (redefine [model] and [acc_val] somewhere) #####\n",
        "\n",
        "    learning_rate = hyperparameters.get('learning_rate')\n",
        "    batch_size = hyperparameters.get('batch_size')\n",
        "    num_epochs = hyperparameters.get('num_epochs')\n",
        "\n",
        "    trainloader = DataLoader(data_tr, batch_size = batch_size, shuffle=True)\n",
        "    validloader = DataLoader(data_val, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr = learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, learning_rate, num_epochs* len(data_tr))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      model, optimizer, avg_loss = one_epoch(model, optimizer, trainloader)\n",
        "      acc_val, loss_val = validate(model, validloader)\n",
        "      scheduler.step()\n",
        "\n",
        "    ##### YOUR CODE ENDS HERE   ################################################\n",
        "    return model, acc_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46UtCZJCFRXf",
        "outputId": "a24250ad-1f48-4204-ee42-f92d7d4f39cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "0.6312441349983215 70.14\n",
            "0.5676066842556 72.87\n",
            "0.5437824920415878 74.0\n",
            "0.5258740147590637 74.63\n",
            "0.5135277182340622 75.44\n",
            "0.5036107137441636 75.75\n",
            "0.495849379324913 77.17\n",
            "0.4871834241390228 77.35\n",
            "0.4827282478094101 76.73\n",
            "0.47520403723716736 77.95\n",
            "0.47200482671260835 78.66\n",
            "0.46749858434200287 78.77\n",
            "0.46595187747478484 77.11\n",
            "0.4561788381576538 79.69000000000001\n",
            "0.4563820033073425 79.43\n",
            "0.45225237033367155 79.22\n",
            "0.4508535409450531 80.08999999999999\n",
            "0.4456490130662918 78.36999999999999\n",
            "0.44646070294380186 79.84\n",
            "0.4429484364748001 79.47\n",
            "0.4379809746265411 80.67999999999999\n",
            "0.43423329815864564 80.60000000000001\n",
            "0.43150466537475585 81.45\n",
            "0.43538962285518645 81.11\n",
            "0.43167245552539824 80.66\n",
            "0.42726236174106597 81.35\n",
            "0.41865457541942597 81.23\n",
            "0.4227517702817917 81.43\n",
            "0.42375799884796145 80.21000000000001\n",
            "0.42017695264816285 81.92\n",
            "0.4149860407114029 81.67999999999999\n",
            "0.4206607741594315 82.1\n",
            "0.41793514032363893 81.6\n",
            "0.41507954103946687 81.6\n",
            "0.4110004977941513 81.82000000000001\n",
            "0.4084595145940781 81.81\n",
            "0.4099633640527725 81.94\n",
            "0.40748118472099304 82.19999999999999\n",
            "0.40398050862550733 81.92\n",
            "0.4046333739757538 82.98\n",
            "0.4055213772058487 82.57\n",
            "0.40468628019094466 82.50999999999999\n",
            "0.4002015545845032 82.32000000000001\n",
            "0.3970693153381348 82.37\n",
            "0.39862038390636445 82.50999999999999\n",
            "0.3973650841474533 82.69999999999999\n",
            "0.39929268851280214 82.82000000000001\n",
            "0.3990656274318695 83.13000000000001\n",
            "0.39995632092952726 82.86\n",
            "0.3940215092539787 83.16\n",
            "0.3907181427717209 82.48\n",
            "0.3909901161432266 82.66\n",
            "0.39321110776662827 83.11\n",
            "0.3904983209013939 82.88\n",
            "0.3895602456450462 83.45\n",
            "0.38689491770267487 83.13000000000001\n",
            "0.38544329149723056 82.89\n",
            "0.3895304486632347 83.25\n",
            "0.3854190402030945 82.46\n",
            "0.3864700352072716 83.35000000000001\n",
            "0.3859460906624794 83.31\n",
            "0.38314254702329636 83.17\n",
            "0.3868128205180168 83.82\n",
            "0.3870288793683052 83.55\n",
            "0.38088380001783373 83.95\n",
            "0.38291433662176133 83.35000000000001\n",
            "0.3741407167315483 83.27\n",
            "0.37757316353321074 83.88\n",
            "0.3784162289619446 83.23\n",
            "0.3776868777513504 84.0\n",
            "0.37587002509832385 83.83\n",
            "0.3803987975358963 83.31\n",
            "0.37853372468948365 83.22\n",
            "0.37528529105186464 83.58\n",
            "0.3792148894071579 83.27\n",
            "0.370787862598896 83.3\n",
            "0.3772050736427307 83.6\n",
            "0.3725065515756607 84.47\n",
            "0.37145538934469224 83.85000000000001\n",
            "0.37481519111394884 84.42\n",
            "0.37042604328393935 82.42\n",
            "0.3687404079198837 84.07\n",
            "0.36760136038064956 84.21\n",
            "0.37160818746089935 84.65\n",
            "0.364969918358326 84.43\n",
            "0.36901634261608124 83.88\n",
            "0.3661253987312317 84.44\n",
            "0.3648001085400581 84.78\n",
            "0.36306086713075636 83.56\n",
            "0.3659083897829056 84.55\n",
            "0.3636656203508377 84.58\n",
            "0.36200616062879565 84.19\n",
            "0.36375326105356215 84.64\n",
            "0.3603229311823845 84.66\n",
            "0.3623062649011612 84.11\n",
            "0.3603171957969665 85.09\n",
            "0.3631441791653633 84.38\n",
            "0.35998040482997895 84.77\n",
            "0.3587630052924156 84.89999999999999\n",
            "0.35749429869651794 83.35000000000001\n"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "# Do hyperparameter search using train_and_validate(). You should call\n",
        "# generate_test_predictions() on the model you eventually compute as best to \n",
        "# get test predictions to submit to Kaggle. You should also split off some of\n",
        "# the \n",
        "data = build_dataset()  # You should use this data for training and validation\n",
        "best_model = NN()       # You should at some point name the model you want to\n",
        "                        # generate test predictions with `best_model`\n",
        "                        \n",
        "##### YOUR CODE STARTS HERE ####################################################\n",
        "\n",
        "trainset = data\n",
        "testset = data\n",
        "\n",
        "hyperparameters = {\"learning_rate\":0.001, \"batch_size\": 16, \"num_epochs\": 100}\n",
        "best_model, acc_val = train_and_validate(trainset, testset, **hyperparameters)\n",
        "\n",
        "##### YOUR CODE ENDS HERE   ####################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYeTSbWP-o3m",
        "outputId": "b3cf687c-e41e-4065-92db-a051406f61b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote model predictions to test_predictions.csv\n"
          ]
        }
      ],
      "source": [
        "generate_test_predictions(best_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CMPT726_419_A3_Q4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}